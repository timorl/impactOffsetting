\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\newcommand{\stateSpace}{\mathcal{S}}
\newcommand{\actionSpace}{\mathcal{A}}
\newcommand{\rewardSpace}{\mathcal{R}}
\newcommand{\trasitionFunction}{T}
\newcommand{\rewardFunction}{R}
\newcommand{\modifiedRewardFunction}{\rewardFunction_{\mathrm{SEA}}}
\newcommand{\generalRewardFunction}{\rewardFunction_{\mathrm{General}}}
\newcommand{\inactionRewardFunction}{\rewardFunction_{\mathrm{Inaction}}}
\newcommand{\corruption}{C}
\newcommand{\deviationFromBaseline}{d}
\newcommand{\baseline}{b}
\newcommand{\tuple}[1]{\left\langle #1 \right\rangle}
\newcommand{\rawMDP}{\stateSpace, \actionSpace, \rewardSpace, \trasitionFunction, \rewardFunction}
\newcommand{\MDP}{\tuple{\rawMDP}}
\newcommand{\rawCRMDP}{\rawMDP, \corruption}
\newcommand{\CRMDP}{\tuple{\rawCRMDP}}
\newcommand{\rawSCRMDP}{\rawCRMDP, d, \LV}
\newcommand{\SCRMDP}{\tuple{\rawSCRMDP}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\absoluteValue}[1]{\left\lvert #1 \right\rvert}

\newtheorem{example}{Example}

\title{Impact offsetting}
\author{Us}
\date{2021}

\begin{document}

\maketitle

\section{Setting}

 To define offsetting lets first formalize the notion of a misspecified reward. I'm using the below formulation because I already had it lying around.

	\begin{definition}[CRMDP]
		A \emph{Corrupt Reward Markov Decision Process} (CRMDP) is a tuple $\CRMDP$, where $\stateSpace$ is the set of states,
		$\actionSpace$ is the set of actions, $\rewardSpace$ is the set of rewards, $\trasitionFunction \colon \stateSpace \times \actionSpace \to P\left( \stateSpace \right)$
		is the transition function, $\rewardFunction \colon \stateSpace \to \rewardSpace$ is the reward function and $\corruption \colon \stateSpace \to \rewardSpace$ is the corrupt
		reward. We call $\MDP$ the \emph{underlying MDP} of a given CRMDP.
		\label{def:CRMDP}
	\end{definition}

	To model having imprecise rewards we assume we only have access to $\corruption$, while we actually care about the real reward, which we are not able to specify.

	One approach to mitigating the problems arising from the discrepancy is side effect avoidance. We assume that the corrupt reward somewhat correctly specifies the actual goal,
	but the negative effects of changing the environment are underspecified. Then problematic behaviour might be reduced by correcting for avoiding any side effects of the agent's
	actions, compared to some baseline. This, however, introduces other biases, namely towards the baseline. We call behaviour stemming from these biases ``offsetting''.

	\begin{definition}[Side effect avoidance]
		\emph{Side effect avoidance} is a class of approaches for mitigating reward misspecification, which
		modify the reward passed to the agent according to the equation
		\begin{equation}
			\modifiedRewardFunction\left(  s_t \right) =
			\corruption\left( s_t \right) -
			\lambda \cdot \deviationFromBaseline\left( s_t, \baseline\left( s_t \right) \right),
			\label{eq:sideEffectAvoidance}
		\end{equation}
		where $t$ is the time step, $s_i$ is the state at time $i$, $\baseline$ is a probability
		distribution of states of the baseline agent, and $\deviationFromBaseline$ is a measure
		of the deviation between a state and a baseline distribution of states.
		\label{def:SEA}
	\end{definition}
	In practice the above definition should be non-Markovian, see appendix.
	In short, $\baseline$ either depends on the initial state and the time step
	(length of trajectory) (Krakovna), or only on the previous state (Turner).

	The deviation $\deviationFromBaseline$ is quite an involved metric in practice,
	usually depending on	relative state	reachability.

	Let us define one more concept which will be useful later.

	\begin{definition}[Baseline reward]
		Let $\baseline \colon \stateSpace \to P\left( \stateSpace \right)$ be a fixed baseline function.
		For any reward function $\rewardFunction$ we write
		\begin{equation*}
			\rewardFunction^\baseline\left( s \right) =
			\mathbb{E}_{s' \in \baseline\left( s \right)} \rewardFunction\left( s' \right).
		\end{equation*}
		\label{def:baselineReward}
	\end{definition}
	The baseline function is supposed to encode what state we expect the agent following
	the baseline policy would have reached at the point when the actual agent reached state $s$.

\section{Offsetting}

	Let us consider two policies. The policy optimal with respect to the base available
	(corrupted) reward $\pi_\corruption$, and the policy optimal with respect to the modified
	reward $\pi_{\modifiedRewardFunction}$.
	We can now define a broad version of offsetting.
	\begin{definition}[Broad offsetting]
		We call the behaviour of an agent following $\pi_{\modifiedRewardFunction}$ that differs
		from the behaviour of an agent following $\pi_\corruption$ \emph{broad offsetting}.
		\label{def:broadOffsetting}
	\end{definition}
	Note that under this definition of offsetting we obviously desire \emph{some} offsetting,
	this was the whole goal of introducing the modified reward in the first place.

	This definition of offsetting also includes interventions in the environment as types
	of offsetting, e.g. in the Sushi example with the baseline being the initial state repeated
	forever and any reasonable deviation measure moving the sushi from the belt is broad offsetting.

	I am under the impression that some researchers understand offsetting differently,
	more based on time -- e.g. Krakovna's formulates it as ``undoing actions''.
	I propose that this definition cannot be made consistent with the examples used
	and some other natural ones.

	\begin{example}[Safety door]
		An agent's task is to turn on a machine in a factory. The factory has a safety door that
		closes whenever the machine is engaged. For some deviation measures the agent will be
		incentivized to block the door from closing if it is impossible to open it afterwards.

		This is clearly not ``undoing'' an action, but calling it offsetting seems appropriate.
	\label{eg:factory}
	\end{example}

	From now I will just write offsetting, omitting the \emph{broad} qualifier.

	\subsection{Desirable vs undesirable offsetting}
		Under the above definition it is clear that some offsetting is desirable, while some is not.
		The formal distinction here is as obvious as useless -- desirable offsetting consists of these
		differences which increase the real reward, while undesirable
		offsetting decreases it. The challenge lies in categorizing these types of offsetting
		without access to the real reward. This requires some assumptions about $\corruption$, as
		well as the baseline.

		\begin{definition}[General reward function]
			Let us assume a completely utilitarian framework. In this framework the creator
			of an agent has a clear utility function over all states and would like the
			agent to act according to this utility function. Let us call this function the
			\emph{general reward function} and denote it $\generalRewardFunction$.

			I am terribly sorry for dismissing the phiosophical problems with the above
			for the purposes of this discussion.
			\label{def:morality}
		\end{definition}

		We obviously cannot specify $\generalRewardFunction$ fully, it is not clear anyopne
		actually has such a view of what they want, and encoding it as a utility function is
		a separate challenge that	is also mostly untouched. 
		%maybe need some citations with actual attempts?

		On the other hand we \emph{can} bound the changes to that function -- just by assuming
		it is a utility function on states we made the approach pretty obvious: minimize the
		changes to the states. This is exactly the route side effect avoidance takes.

		\begin{definition}[Inaction reward function]
			We call a reward that tries to minimize the effect on $\generalRewardFunction$ of an agent
			by minimizing changes to states an \emph{inaction reward function} and denote it by
			$\inactionRewardFunction$.
			\label{def:inactionReward}
		\end{definition}

		Note that in the case of side effect avoidance approaches this is the second term of
		\eqref{eq:sideEffectAvoidance}, i.e.
		\begin{equation*}
			\inactionRewardFunction\left( s \right) =
			-\lambda \cdot \deviationFromBaseline\left( s, \baseline\left( s \right) \right).
		\end{equation*}
		This is a family of inaction reward functions and the arbitrariness of the choice of $\lambda$,
		$\deviationFromBaseline$, and $\baseline$ means it is not in any way canonical.

		Let us denote the reward function we give the agent to optimize for
		\begin{equation*}
			\rewardFunction = \corruption + \inactionRewardFunction.
		\end{equation*}
		Note that if we have a goal-independent inaction reward function, we only have to specify
		$\corruption$, which is exactly what we wanted in the first place.

		If we assume we picked the baseline policy such that an agent behaving according to it
		is an acceptable fail-case for us, we can see that we would like the agent to only behave
		differently than the baseline if the general reward of its trajectory is higher than
		the reward over the baseline. We can formalize a somewhat stronger version of this assumption
		in a way that makes it somewhat useful for designing rewards provided to the agents.

		\begin{definition}[Fallback compatible reward function]
			For a fixed $\inactionRewardFunction$, we call $\corruption$ \emph{fallback compatible} if
			\begin{equation}
				\generalRewardFunction - \generalRewardFunction^\baseline \geq
				\corruption + \inactionRewardFunction -
				\corruption^\baseline - \inactionRewardFunction^\baseline.
			\label{eq:fallbackCompatible}
			\end{equation}
			\label{def:fallbackCompatible}
		\end{definition}

		It is trivial to see that this condition implies that a policy can be optimal with respect to
		$\rewardFunction$ only if it is either visiting the states within the baseline with the same
		frequency as the baseline	or states that actually have higher general reward than the baseline
		states. After all if $\rewardFunction$ is higher at a state than at the corresponding baseline,
		the above condition implies so is the real reward.

		For the purpose of designing reward functions it might be convenient to assume
		that $\corruption$ is zero at the baseline. Condition \eqref{eq:fallbackCompatible}
		becomes somewhat more tractable then
		\begin{equation*}
				\generalRewardFunction - \generalRewardFunction^\baseline \geq
				\corruption + \inactionRewardFunction - \inactionRewardFunction^\baseline.
		\end{equation*}
		The easiest way to follow it is to ensure all states encountered
		when following the baseline have zero reward, which should often be doable.

		We could demand this to be true also for $\inactionRewardFunction$, but the easiest way of
		doing this I can come up with is subtracting the expectation over the baseline from it.
		This should actually be quite doable in practice, but no one currently does it.
		Is it worth it just for the simplicity of equations? Who knows.

		Krakovna's position seems to be that choosing $\corruption$ carefully we can eliminate
		all undesirable offsetting. This is clearly not true in general, as we would have to specify
		it exactly in accordance with $\generalRewardFunction$, but it might be true within
		a specific task, which can be seen as a component of $\generalRewardFunction$. In this
		approach we identify what component we care about for a specific task, and define
		$\corruption$ to be that component plus a term nullifying the inaction reward
		with respect to that component. If we managed to make $\inactionRewardFunction$
		into an exact reward penalizing changes to $\generalRewardFunction$, this would just be
		the component times two, but that is impractical to expect.

		The important thing we achieved here is that if we create rewards so that they are fallback
		compatible, the agents we create will not cause more harm than they would have if they
		just followed the baseline policy.

\section{Stochastic side effect avoidance}
	This section presents an as of yet unexplored choice for $\baseline$.
	\subsection{Basic idea}
	 We would like to answer the counterfactual question ``What would the world look like if we used
		the baseline policy rather than the policy we actually used?''.
	 A solution to this problem would be a generator of baseline states that would,
		given an arbitrary state $s$, produce a distribution of states that could have
		been encountered instead of $s$.

		There is an obvious degree of freedom in the counterfactual question,
		even before we get to specifying what the generator should be doing.
		When should the counterfactual intervention happen? The answer to this
		question is the main difference between Krakovna's and Turner's approaches
		-- Krakovna answers ``at the beginning of time'', while Turner ``just a step ago''.
		Both these answers introduce some problems. I would argue we should be looking at
		a discounted combination of counterfactual interventions, i.e.
		\begin{equation}
			\baseline\left( s_T \right) = \frac{1-\gamma}{\gamma} \sum_{i=1}^T \gamma^{i} \baseline_{s_{T-i}}\left( s_T \right)
			\label{eq:discountedBaseline}
		\end{equation}
		where $\baseline_s$ corresponds to taking state $s$ as the point of counterfactual intervention.
		In this language Krakovna's baseline is equal to $\baseline_{s_0}$ and Turner's to $\baseline_{s_{T-1}}$.
		For small-scale experiments these baselines could just be results of parallel rollouts,
		but generators for them, or even the combined baseline, can in principle also be trained.
	\subsection{Advantages -- policy vs trajectory baseline}
	 This approach lessens the problem with Turner immediately accepting any loss in reversability
		after the agent is one step away from the decision that caused it. Such losses still
		become less relevant over time, but in a much more gradual way.

		The comparison with Krakovna is more involved. This approach definitely answers some problems
		related to what ``beginning of episode'' might mean, but in itself it's not a strong reason to
		favour it. For a better reason one has to notice that Krakovna \emph{de facto} uses a baseline
		\emph{trajectory} rather than a baseline \emph{policy}. To illustrate why this distinction
		matters let us discuss an environment:
		\begin{example}[Charging stations]
			Consider a gridworld in the shape of a 5 by 5 square. Points $\left( 0,0 \right)$,
			$\left( 0,2 \right)$, $\left( 4,0 \right)$ contain ``charging stations''. The agent is tasked
			with picking up pieces of trash randomly appearing in other places on the map.
		\end{example}
		This is only a very basic example to illustrate the problems. Also note that it is stochastic --
		this could be avoided, but the added complications would just muddy the waters.

		A natural safe baseline for this environment consists of moving to the closest charging station
		and staying there. However, if the agent originally starts in a specific point (say, $\left( 0,0 \right)$),
		an agent following Krakovna's approach will be incentivized to always go to that point.
		The baseline trajectory is always at this point, and the discounting in the future task measure
		means that the agent will be comparatively penalized for staying at any other point.

		The natural idea to fix it, randomizing the starting state, doesn't quite work. Since the starting states
		are not symmetric, a uniform distribution over the charging stations introduces at least an incentive
		to stay away from point $\left( 4,0 \right)$. A uniform distribution over all points is harder
		to analyse, but the way the resulting behaviour depends on the exact shape of the environment
		means it will be impossible to consistently recover the intended baseline policy.

		In contrast Turner's approach looks at the last state, thus picking behaviour that makes sense locally.
		Similarly the discounted combined baseline, although it has some lingering effects from states further
		back in time.

		This problem in general happens for any situation in which the dynamic system induced by the baseline
		policy has a set of attractors different from a single point. Then any single trajectory ends up
		in one of these attractors, and incentives in Krakovna's approach are skewed towards going to that
		attractor. Another relatively natural example of such situation is a baseline with a patrol route ––
		in Krakovna's approach the agent would be penalized for being out of sync with the patrol
		corresponding to the starting state.

		Krakovna mentioned that extending her methods to stochastic environments is one possible
		avenue of future research. She wasn't quite talking about this specific problem, but I suspect
		the underlying reason for problems in stochastic environments might be the same. In particular,
		the example she uses also talks about stochastically combining trajectories being unsatisfactory,
		and I believe the combined baseline approach should actually work in the sometimes-human-opens-window
		environment.

	\subsection{CVAE}
		The generator for the combined baseline could be trained on random policies with
		baseline policy rollouts	computed starting from past states. For tractability it might
		be better to sample fewer point baselines, e.g. only one every $4^k$ steps. This requires
		some rescaling of the discount factors which will be left as an exercise to the implementer.

		A relatively natural approach to this problem is to use a slightly modified CVAE.
		We create a rollout of a random policy $\pi$, of the form $\left( s_0, \dots s_T \right)$.
		We also create rollouts of the baseline policy $\pi_b$ of the forms
		\begin{equation*}
			\left( s^{\left( m \right)}_0 = s_m, \dots, s^{\left( m \right)}_{T-m} \right).
		\end{equation*}
		For every $t$ we use $s_t$ as an input to the CVAE, backpropagating a combination of
		$k$ losses
		\begin{equation*}
			L(s_t, s^{\left( t-1 \right)}_{1}, s^{\left( t-4 \right)}_4, \dots, s^{\left( t-4^k \right)}_{4^k}) = \sum_{l = 0}^k 2^{-l}L_{CVAE}\left( s_t, s^{\left( t-4^l \right)}_{4^l} \right)
		\end{equation*}
		Where $L_{CVAE}$ is the standard CVAE loss.

		(It might be preferable to actually train $k$ different models, but that is likely to
		be prohibitively costly and depending on the baseline policy might not be that much more detailed.
	 Whether this kind of combined training actually converges to something is not quite obvious though.)

		The resulting generator could then be used as $\baseline$ with an arbitrary side effect avoidance
		measure, like relative reachability. For a given state it should be likely to return
		a baseline state corresponding to a possible rollout of the baseline policy from possibly some
		steps in the past.

		\subsubsection{Doubts}
		 Does this actually work?
			\begin{enumerate}
				\item Does training with $L$ actually create a generator that
					generates things mostly corresponding to the $l$th argument with about $2^{-l}$
					probability? Or does this converge to something else, e.g. always the $0$th argument?
				\item Is it reasonable expect that the CVAE model will be able to ``model'' the whole
					transformation needed to get from $s_t$ to $s^{\left( t-4^l \right)}_{4^l}$?
					After all this requires going back $4^l$ steps and back forward using a different policy.
					Even if we had one model per $l$ this might be a nontrivial task. On the other hand this might
					be simpler if the baseline policy and undisturbed environment behaviour is simple.
				\item Variant of the above -- if we use multiple models we could have them generate
					partial (as in only some steps) trajectories. This I at least know is mostly possible
					and might improve accuracy and interpretability.
				\item What about parts of the state space that random policies cannot reach, but the
					actual policy used by the training agent can. How well will this generalize?
			\end{enumerate}
\section{Are we trying to solve two problems at once?}
	The approaches used by Krakovna and Turner, due to their choices for $\deviationFromBaseline$,
	can be summed up as ``Avoid increasing power compared to the baseline.''. This is a pretty natural
	result of trying to avoid both breaking vases (decreasing everyone's power) and stealing sushi
	(increasing the agent's power at the cost of the hungry human). However, it has the inconvenient
	consequence of making us work with nonlinear functions, something everybody hates.
	An alternative approach is to treat this problem as two separate problems.

	First problem, we want the agent to have power. If it is incentivized to seek power in general,
	for example using the future tasks paradigm (but not relative to any baseline), it will avoid
	performing irreversible actions, such as breaking vases. A natural transformation of the approaches
	of Krakovna and Turner to solving this problem ends up massively simpler than side effect avoidance.
	We just have to add a term for increasing power to the reward, using future tasks would be linear,
	and we wouldn't even have to worry about any baselines.

	Second problem, avoiding interference with other agents. Achieving any specific goal almost always
	requires sacrificing some power, like eating sushi prevents you from using it later. Agents will
	thus routinely lower everyone's power when achieving their goals. An agent incentivized to increase
	power would also be incentivized to interfere with all other agents. Preventing this is much harder
	than the solution to the previous problem, consider for example that agent $A$ cannot even rely
	on agent $B$'s presence as a clue to whether a specific power-lowering phenomenon is intended by $B$.
	This would work for eating sushi, but not for automated machinery set up in a factory before agent $A$
	existed. Considering how much simpler the solution to the first problem is than side effect avoidance,
	investigating this one might still be worthwhile.

	Note that the approach intensifies a problem in AI alignment commonly mentioned since the very
	inception of the field -- power seeking is an instrumental goal for sufficiently general
	agents anyway, so it seems unwise to additionally encourage it.

	On the other hand this might actually increase the capabilities of agents, by explicitly introducing
	incentives we expect in more capable agents.
	Has anyone checked that? Worrying thought.
\appendix
	\section{Non-Markovian rewards}
	 In the main text I pretended everything is Markovian, but at least Krakovna's original approach
		is not. A definition that would actually apply to her appraoch is presented below,
		but was too unwieldy to work with and did not actually provide much insight.
		\begin{definition}[Side effect avoidance]
			\emph{Side effect avoidance} is a class of approaches for mitigating reward misspecification, which
			modify the reward passed to the agent according to the equation
			\begin{equation}
				\modifiedRewardFunction\left( \left( s_i \right)_{i=0}^t \right) =
				\corruption\left( s_t \right) -
				\lambda \cdot \deviationFromBaseline\left( \left( s_i \right)_{i=0}^t, \baseline\left( \left( s_i \right)_{i=0}^t \right) \right),
				\label{eq:sideEffectAvoidance-nonMarkov}
			\end{equation}
			where $t$ is the time step, $s_i$ is the state at time $i$, $\baseline$ is a probability
			distribution of trajectories of the baseline agent, and $\deviationFromBaseline$ is a measure
			of the deviation between a trajectory and a baseline distribution of trajectories.

			In practice $\baseline$ either depends on the initial state and the time step
			(length of trajectory) (Krakovna), or only on the previous state (Turner)	and not both.

			The deviation $\deviationFromBaseline$ is a more involved metric, usually depending on
			relative state	reachability. All approaches up to now only used the last state
			in the trajectories under consideration.
			\label{def:SEA-nonMarkov}
		\end{definition}
		The usual approaches of making environments Markovian could work to completely avoid referring
		to this definition.
\end{document}

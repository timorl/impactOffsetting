\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{todonotes}
\usepackage{enumerate}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{algpseudocode}
\usepackage{algorithm}

\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

\newcommand{\stateSpace}{\mathcal{S}}
\newcommand{\actionSpace}{\mathcal{A}}
\newcommand{\rewardSpace}{\mathcal{R}}
\newcommand{\trasitionFunction}{T}
\newcommand{\rewardFunction}{R}
\newcommand{\modifiedRewardFunction}{\rewardFunction_{\mathrm{SEA}}}
\newcommand{\generalRewardFunction}{\rewardFunction_{\mathrm{General}}}
\newcommand{\inactionRewardFunction}{\rewardFunction_{\mathrm{Inaction}}}
\newcommand{\corruption}{C}
\newcommand{\deviationFromBaseline}{d}
\newcommand{\baseline}{b}
\newcommand{\tuple}[1]{\left\langle #1 \right\rangle}
\newcommand{\rawMDP}{\stateSpace, \actionSpace, \rewardSpace, \trasitionFunction, \rewardFunction}
\newcommand{\MDP}{\tuple{\rawMDP}}
\newcommand{\rawCRMDP}{\rawMDP, \corruption}
\newcommand{\CRMDP}{\tuple{\rawCRMDP}}
\newcommand{\rawSCRMDP}{\rawCRMDP, d, \LV}
\newcommand{\SCRMDP}{\tuple{\rawSCRMDP}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\absoluteValue}[1]{\left\lvert #1 \right\rvert}

\newtheorem{example}{Example}

\title{Impact offsetting}
\author{Us}
\date{2021}

\begin{document}

\maketitle

\section{Setting}

 To define offsetting lets first formalize the notion of a misspecified reward. I'm using the below formulation because I already had it lying around.

	\begin{definition}[CRMDP]
		A \emph{Corrupt Reward Markov Decision Process} (CRMDP) is a tuple $\CRMDP$, where $\stateSpace$ is the set of states,
		$\actionSpace$ is the set of actions, $\rewardSpace$ is the set of rewards, $\trasitionFunction \colon \stateSpace \times \actionSpace \to P\left( \stateSpace \right)$
		is the transition function, $\rewardFunction \colon \stateSpace \to \rewardSpace$ is the reward function and $\corruption \colon \stateSpace \to \rewardSpace$ is the corrupt
		reward. We call $\MDP$ the \emph{underlying MDP} of a given CRMDP.
		\label{def:CRMDP}
	\end{definition}

	To model having imprecise rewards we assume we only have access to $\corruption$, while we actually care about the real reward, which we are not able to specify.

	One approach to mitigating the problems arising from the discrepancy is side effect avoidance. We assume that the corrupt reward somewhat correctly specifies the actual goal,
	but the negative effects of changing the environment are underspecified. Then problematic behaviour might be reduced by correcting for avoiding any side effects of the agent's
	actions, compared to some baseline. This, however, introduces other biases, namely towards the baseline. We call behaviour stemming from these biases ``offsetting''.

	\begin{definition}[Side effect avoidance]
		\emph{Side effect avoidance} is a class of approaches for mitigating reward misspecification, which
		modify the reward passed to the agent according to the equation
		\begin{equation}
			\modifiedRewardFunction\left(  s_t \right) =
			\corruption\left( s_t \right) -
			\lambda \cdot \deviationFromBaseline\left( s_t, \baseline\left( s_t \right) \right),
			\label{eq:sideEffectAvoidance}
		\end{equation}
		where $t$ is the time step, $s_i$ is the state at time $i$, $\baseline$ is a probability
		distribution of states of the baseline agent, and $\deviationFromBaseline$ is a measure
		of the deviation between a state and a baseline distribution of states.
		\label{def:SEA}
	\end{definition}
	In practice the above definition should be non-Markovian, see appendix.
	In short, $\baseline$ either depends on the initial state and the time step
	(length of trajectory) (Krakovna), or only on the previous state (Turner).

	The deviation $\deviationFromBaseline$ is quite an involved metric in practice,
	usually depending on	relative state	reachability.

	Let us define one more concept which will be useful later.

	\begin{definition}[Baseline reward]
		Let $\baseline \colon \stateSpace \to P\left( \stateSpace \right)$ be a fixed baseline function.
		For any reward function $\rewardFunction$ we write
		\begin{equation*}
			\rewardFunction^\baseline\left( s \right) =
			\mathbb{E}_{s' \in \baseline\left( s \right)} \rewardFunction\left( s' \right).
		\end{equation*}
		\label{def:baselineReward}
	\end{definition}
	The baseline function is supposed to encode what state we expect the agent following
	the baseline policy would have reached at the point when the actual agent reached state $s$.

\section{Offsetting}

	Let us consider two policies. The policy optimal with respect to the base available
	(corrupted) reward $\pi_\corruption$, and the policy optimal with respect to the modified
	reward $\pi_{\modifiedRewardFunction}$.
	We can now define a broad version of offsetting.
	\begin{definition}[Broad offsetting]
		We call the behaviour of an agent following $\pi_{\modifiedRewardFunction}$ that differs
		from the behaviour of an agent following $\pi_\corruption$ \emph{broad offsetting}.
		\label{def:broadOffsetting}
	\end{definition}
	Note that under this definition of offsetting we obviously desire \emph{some} offsetting,
	this was the whole goal of introducing the modified reward in the first place.

	This definition of offsetting also includes interventions in the environment as types
	of offsetting, e.g. in the Sushi example with the baseline being the initial state repeated
	forever and any reasonable deviation measure moving the sushi from the belt is broad offsetting.

	I am under the impression that some researchers understand offsetting differently,
	more based on time -- e.g. Krakovna's formulates it as ``undoing actions''.
	I propose that this definition cannot be made consistent with the examples used
	and some other natural ones.

	\begin{example}[Safety door]
		An agent's task is to turn on a machine in a factory. The factory has a safety door that
		closes whenever the machine is engaged. For some deviation measures the agent will be
		incentivized to block the door from closing if it is impossible to open it afterwards.

		This is clearly not ``undoing'' an action, but calling it offsetting seems appropriate.
	\label{eg:factory}
	\end{example}

	From now I will just write offsetting, omitting the \emph{broad} qualifier.

	\subsection{Desirable vs undesirable offsetting}
		Under the above definition it is clear that some offsetting is desirable, while some is not.
		The formal distinction here is as obvious as useless -- desirable offsetting consists of these
		differences which increase the real reward, while undesirable
		offsetting decreases it. The challenge lies in categorizing these types of offsetting
		without access to the real reward. This requires some assumptions about $\corruption$, as
		well as the baseline.

		\begin{definition}[General reward function]
			Let us assume a completely utilitarian framework. In this framework the creator
			of an agent has a clear utility function over all states and would like the
			agent to act according to this utility function. Let us call this function the
			\emph{general reward function} and denote it $\generalRewardFunction$.

			I am terribly sorry for dismissing the phiosophical problems with the above
			for the purposes of this discussion.
			\label{def:morality}
		\end{definition}

		We obviously cannot specify $\generalRewardFunction$ fully, it is not clear anyopne
		actually has such a view of what they want, and encoding it as a utility function is
		a separate challenge that	is also mostly untouched. 
		%maybe need some citations with actual attempts?

		On the other hand we \emph{can} bound the changes to that function -- just by assuming
		it is a utility function on states we made the approach pretty obvious: minimize the
		changes to the states. This is exactly the route side effect avoidance takes.

		\begin{definition}[Inaction reward function]
			We call a reward that tries to minimize the effect on $\generalRewardFunction$ of an agent
			by minimizing changes to states an \emph{inaction reward function} and denote it by
			$\inactionRewardFunction$.
			\label{def:inactionReward}
		\end{definition}

		Note that in the case of side effect avoidance approaches this is the second term of
		\eqref{eq:sideEffectAvoidance}, i.e.
		\begin{equation*}
			\inactionRewardFunction\left( s \right) =
			-\lambda \cdot \deviationFromBaseline\left( s, \baseline\left( s \right) \right).
		\end{equation*}
		This is a family of inaction reward functions and the arbitrariness of the choice of $\lambda$,
		$\deviationFromBaseline$, and $\baseline$ means it is not in any way canonical.

		Let us denote the reward function we give the agent to optimize for
		\begin{equation*}
			\rewardFunction = \corruption + \inactionRewardFunction.
		\end{equation*}
		Note that if we have a goal-independent inaction reward function, we only have to specify
		$\corruption$, which is exactly what we wanted in the first place.

		If we assume we picked the baseline policy such that an agent behaving according to it
		is an acceptable fail-case for us, we can see that we would like the agent to only behave
		differently than the baseline if the general reward of its trajectory is higher than
		the reward over the baseline. We can formalize a somewhat stronger version of this assumption
		in a way that makes it somewhat useful for designing rewards provided to the agents.

		\begin{definition}[Fallback compatible reward function]
			For a fixed $\inactionRewardFunction$, we call $\corruption$ \emph{fallback compatible} if
			\begin{equation}
				\generalRewardFunction - \generalRewardFunction^\baseline \geq
				\corruption + \inactionRewardFunction -
				\corruption^\baseline - \inactionRewardFunction^\baseline.
			\label{eq:fallbackCompatible}
			\end{equation}
			\label{def:fallbackCompatible}
		\end{definition}

		It is trivial to see that this condition implies that a policy can be optimal with respect to
		$\rewardFunction$ only if it is either visiting the states within the baseline with the same
		frequency as the baseline	or states that actually have higher general reward than the baseline
		states. After all if $\rewardFunction$ is higher at a state than at the corresponding baseline,
		the above condition implies so is the real reward.

		For the purpose of designing reward functions it might be convenient to assume
		that $\corruption$ is zero at the baseline. Condition \eqref{eq:fallbackCompatible}
		becomes somewhat more tractable then
		\begin{equation*}
				\generalRewardFunction - \generalRewardFunction^\baseline \geq
				\corruption + \inactionRewardFunction - \inactionRewardFunction^\baseline.
		\end{equation*}
		The easiest way to follow it is to ensure all states encountered
		when following the baseline have zero reward, which should often be doable.

		We could demand this to be true also for $\inactionRewardFunction$, but the easiest way of
		doing this I can come up with is subtracting the expectation over the baseline from it.
		This should actually be quite doable in practice, but no one currently does it.
		Is it worth it just for the simplicity of equations? Who knows.

		Krakovna's position seems to be that choosing $\corruption$ carefully we can eliminate
		all undesirable offsetting. This is clearly not true in general, as we would have to specify
		it exactly in accordance with $\generalRewardFunction$, but it might be true within
		a specific task, which can be seen as a component of $\generalRewardFunction$. In this
		approach we identify what component we care about for a specific task, and define
		$\corruption$ to be that component plus a term nullifying the inaction reward
		with respect to that component. If we managed to make $\inactionRewardFunction$
		into an exact reward penalizing changes to $\generalRewardFunction$, this would just be
		the component times two, but that is impractical to expect.

		The important thing we achieved here is that if we create rewards so that they are fallback
		compatible, the agents we create will not cause more harm than they would have if they
		just followed the baseline policy.

	\appendix
	\section{Non-Markovian rewards}
	 In the main text I pretended everything is Markovian, but at least Krakovna's original approach
		is not. A definition that would actually apply to her appraoch is presented below,
		but was too unwieldy to work with and did not actually provide much insight.
		\begin{definition}[Side effect avoidance]
			\emph{Side effect avoidance} is a class of approaches for mitigating reward misspecification, which
			modify the reward passed to the agent according to the equation
			\begin{equation}
				\modifiedRewardFunction\left( \left( s_i \right)_{i=0}^t \right) =
				\corruption\left( s_t \right) -
				\lambda \cdot \deviationFromBaseline\left( \left( s_i \right)_{i=0}^t, \baseline\left( \left( s_i \right)_{i=0}^t \right) \right),
				\label{eq:sideEffectAvoidance-nonMarkov}
			\end{equation}
			where $t$ is the time step, $s_i$ is the state at time $i$, $\baseline$ is a probability
			distribution of trajectories of the baseline agent, and $\deviationFromBaseline$ is a measure
			of the deviation between a trajectory and a baseline distribution of trajectories.

			In practice $\baseline$ either depends on the initial state and the time step
			(length of trajectory) (Krakovna), or only on the previous state (Turner)	and not both.

			The deviation $\deviationFromBaseline$ is a more involved metric, usually depending on
			relative state	reachability. All approaches up to now only used the last state
			in the trajectories under consideration.
			\label{def:SEA-nonMarkov}
		\end{definition}
		The usual approaches of making environments Markovian could work to completely avoid referring
		to this definition.
\end{document}
